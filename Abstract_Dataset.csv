abstract,paper
"To understand computational thinking in App Inventor, it is important to be able to effectively evaluate computational complexity in block-based programming languages. In the past, there have been a handful of complexity measures proposed for text-based languages (Weyuker, 1988). In this paper, we will attempt to implement 2 such measures, Halstead?s Programming Effort and statement count, in App Inventor on a dataset of projects from 50 random users. The goal is to determine whether or not text programming standards for complexity can be generalized to block programming languages. This paper shows that the 2 complexity measures we implemented are not adequate measures for complexity in App Inventor. This result indicates a need for different measures of complexity that more accurately portray block programming proficiency. We hope this study will be a gateway into a better understanding of the intricacies of App Inventor?s block programming language and its unique contributions to the development of computational thinking.",Evaluations of Programming Complexity in App Inventor
"This paper presents the results of a preliminary investigation into how the teaching of computational thinking -- particularly algorithmic thinking and programming -- to university undergraduate students varies depending on aptitude and perceived enjoyment of STEM subjects during their secondary-level (pre-university) education. We investigated a specific component of computational thinking, algorithmic thinking, comparing against a student's ability to develop knowledge and understanding of introductory programming.",An investigation into susceptibility to learn computational thinking in postcompulsory education
"Researchers have hypothesized strong connections between Computational Thinking (CT) practices and STEM learning. However, there is a lack of consensus on what constitutes an adequate set of CT knowledge and skills. In this paper, we present an initial framework for evaluating students? CT learning. We introduce the primary CT concepts and practices that students can learn and apply in a learning by modeling environment. Our overall goal is to develop assessments that study the synergy between STEM and CT concepts in K-12 curricula. Towards this end, we discuss the results from a teacher-led classroom study we conducted on STEM- and CT-learning in our CTSiM environment.",Assessing Students? Computational Thinking in a Learning by Modeling Environment
"The importance of Computational Thinking (CT) as a goal of science education is increasingly acknowledged. This study investigates the effect of computationally-enriched science curriculum on students? development of CT practices. Over the course of one school year, biology lessons featuring the exploration of NetLogo models were implemented in the classrooms of three 9th grade biology teachers at an urban public secondary school in the United States. One-hundred thirty-three biology students took both pre- and post-tests that were administered at the beginning and end of the school year. The students? responses to relevant assessment items were coded and scored using rubrics designed to evaluate their mastery of two learning objectives relating to modeling and simulation practices. The first learning objective was to explore the relationship between a system?s parameters and its behavior. The second learning objective was to identify the simplifications made by a model. Each item?s pre- and post-test scores were compared using a Wilcoxon signed-rank test. Results indicate a statistically significant improvement with respect to the second of the two learning objectives, suggesting that the computationallyenriched biology curriculum enhanced students? ability to identify the simplifications made by a model.",Computational Thinking in the Science Classroom
"Computational thinking (CT) practices, especially abstraction and evaluation, are central to developing expertise in scientific disciplines, and considerable synergies exist between CT and scientific expertise. We present a pedagogical model based on the EquationBased Model (EBM) for developing computerized simulations to describe physical phenomena. Specifically, EBM emphasizes the importance of mathematics as a central tool in science, and aims at fostering students? abstraction and evaluation practices, as part of their modeling processes. We analyzed a final team-project of participants who decided to investigate a specific physical phenomenon in a course based on the EBM approach. Our analysis focused on characterizing the abstraction and evaluation practices, and the role they play in the scientific inquiry. The students applied multiple levels of abstraction, starting with the mathematic-system-level perspective of the conceptual model, and eventually constructed a computerized model of the conceptual model. They applied mathematical tools throughout the process, and verified and validated their models. The graphical simulation that the students built enabled them to investigate and enhance their comprehension of the problem explored. We concluded that this pedagogic approach has the potential to promote meaningful learning and knowledge transfer of computational thinking that were acquired during the course.",Constructing Models in Physics: What Computational Thinking Occurs?
"Computational Thinking involves core computer science concepts and practices that apply to multiple disciplines including science and mathematics. Currently, there is a strong drive toward integrating computer science into the K-12 STEM curricula. Several general-purpose programming environments have been developed to support the learning of CT and computing concepts and practices. Domain-specific modeling languages (DSMLs), on the other hand are designed for specific applications in engineering domains. As compared to general-purpose programming languages, DSMLs provide ease of use and more power to express domain-specific concepts, thus increasing productivity in specific application domains. In this paper, we present design guidelines and a design process for constructing DSMLs to facilitate STEM learning by computational modeling. To illustrate the process, we provide a case study of designing a DSML specifically for the kinematics domain.",Domain Specific Modeling Language Design to support Synergistic Learning of STEM and Computational Thinking
"STEM and computer science with the goal of broadening participation in these male-dominated fields. At the same time, the role of computational thinking (CT) as a tool to improve computer science skills along with STEM learning is becoming increasingly significant. This work seeks to add to this research through an analysis of the role confidence in computational thinking plays in developing STEM engagement and abilities. In the study reported in this paper, 40 high school students (21 girls and 19 boys) completed a Scratch project on modeling inelastic collisions in their Physics class. Pre- and post- surveys were conducted to analyze confidence levels in CT. Results showed a statistically significant difference in confidence levels in four CT dimensions: abstraction, flow of control, decomposition, and conditional logic. The results show that boys were more confident than girls in applying each of these dimensions. However, performance on the modeling assignment showed no statistical difference. We discuss the results and its applications to future work.",The Role Gender Differences in Computational Thinking Confidence Levels Plays in STEM Applications
"How is computational thinking education in Germany? This paper aims to investigate computational thinking education in K12 German secondary schools. The methodology is based on analyzing the competence-based curricula frameworks for Computer Science in four highest populated federal states in Germany. In addition to Computer Science education, we also consider other subjects, e.g., Physics, because computational thinking may also exist in other subjects. Finally, we compare computational thinking education in Germany with international level by taking the ACM recommendation for Computer Science curriculum into account.",K-12 Computational Thinking Education in Germany
"This paper addresses two problems which usually occur in learning Mathematics: first, students who face difficulty understanding and are too shy to participate in discussions and subsequently do not manage to resolve their doubts, and second, dull e-learning websites. The many rules in Mathematics compounds the problem further. We thus aim to address these problems through a gamified e-commerceoriented Mathematics learning practice system, Alzebra, for informal learning. Focusing on principles of Information Systems Analysis and Design, e-commerceoriented computational concepts are embedded in the game to motivate online practice. The system concept, design methodology and user testing outcomes are presented. Significance lies in deriving perception towards gamification and components which users liked or disliked and the efficacy of our hybrid approach in systems development.",Gamified Mathematics practice: Designing with e-commerce and computational concepts
"Cloud computing has been growing rapidly since Amazon brought this idea to the public. Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. From the survey conducted by RightScale, 96% of the respondents from different organizations have adopted cloud in various ways. Indeed, cloud computing brings huge advantage to the firm, but moving to the cloud computing is not an easy task. The contribution of this work is to present an overview of cloud computing. Research findings of this paper can be utilized to acquire an understanding of cloud computing and its applications in different sectors.",Cloud Computing Challenges in a General Perspective
Digital IoT technologies present new cyber risk in the supply chain of the digital economy which are often not visible to companies participating in the digital supply chains. This paper discusses how the IoT cyber risks can be visualised in the process of designing business and supply chain strategies. The literature reviewed includes industry and government papers and compares established business and supply chain models with studies on new IoT technologies. This article defines the design parameters for a decision support system for visualising cyber risk from IoT supply chain in the digital economy. The design process is grounded on a case study on two IoT companies. The methods applied in the case study include open and categorical coding and discourse analysis.,Design principles for cyber risk impact assessment from Internet of Things (IoT)
"The new K-12 computing curriculum draft for Taiwan secondary schools was designed to launch in 2018 but the draft only outlined themes and contents for students to learn, without further details on key concepts to be covered in the contents. Therefore, in 2016, a Delphi study was conducted to survey the opinions about what “key learning concepts” should be included for implementation at the secondary level based on the draft. By adopting the Delphi method, different viewpoints from computer scientists and secondary school computing teachers were collected to build consensus of key concepts through a series of convergence. Based on the research results, we found the computer scientists and computing teachers had opposing opinions about whether the secondary school students should learn the advanced concepts. The purpose of this study was to understand the different views on learning concepts of the draft between two groups. The data analyzed in this study were based on the Delphi survey in 2016. This study found computer scientists tended to be more conservative about this issue, therefore they suggested that the advanced and theoretical concepts are not essential at the secondary level, e.g., recursion, searching, sorting, data compression, data conversion, and divide and conquer. This was because the computer scientists considered these concepts as what they had studied in college. Rather, computing teachers knew how to simplify these concepts for teaching at the secondary level. The research findings can serve as useful references for revising and implementing the computing curriculum in the future.",How Computer Scientists and Computing Teachers Think Differently in the Concepts to be Included in a Secondary School Computing Curriculum
Teaching computational thinking can be viewed as cultivating the capacity for logical thinking and problemsolving skills applied to foundational subjects such as mathematics. We report a pilot study on how carefullydesigned mobile app games that gamify elementary algebra learning are used in an annual computer science tournament and also at an annual mathematics festival in Hong Kong. We define mathematics gamification as the process of embedding mathematical concepts and their logical manipulations in a puzzle game-like setting aided by computing technologies. We have evaluated the learning efficacy of our mobile app games to gain numeracy proficiency in an annual computer science tournament for middle school students in Hong Kong.,Teaching Computational Thinking by Gamification of K-12 Mathematics: Mobile App Math Games in Mathematics and Computer Science Tournament
"This paper describes the findings from the Education Development Center’s (EDC) project on Computational Thinking (CT) called “Broadening Participation of Elementary School Teachers and Students in Computer Science through STEM Integration and Statewide Collaboration.” It presents the process used to define the primary job functions and work tasks of a CT Integration Specialist in today’s education settings. Authors describe how the requisite knowledge, skills and practices of the CT integration specialist were assembled and vetted. The article presents ways this profile can be used to guide elementary school teachers in integrating CT into their classrooms and as a framework to guide the development of CT learning activities and assessments, then sets the directions for future work",Profile of a CT Integration Specialist
"To foster new generations becoming creators of technology, CoolThink@JC (“Computational Thinking Education,” n.d.), a four-year project sponsored by Jockey Club Charities Trust in Hong Kong, aims to advocating that “computational thinking is a fundamental skill for everyone” (Wing, 2006, p.33). The project targets to upper primary school students, and in that parent education is one of the components. Particularly, parent education focuses on parent-child relationship in learning computational thinking. Thus, we propose several approaches involving coding hands-on workshop, instructional video learning and unplugged activities to enhance the parent-child mode of learning in a large-scale project involving 32 primary schools in Hong Kong.",Enhancing the Link between Parent-Child in Learning Computational Thinking
"Understanding how teachers promote students’ computational thinking in computer science classes addresses a critical need. We report on how high school teachers implemented a 30-40 hour electronic textile unit in which students designed different wearables with the LilyPad Arduino as part of the Exploring Computer Science curriculum in two classrooms. Our analysis focused on how teachers brought out computational thinking through students’ interactions and projects in three key areas: strategic problem solving, iteration, and interfacing between abstract and tangible computation. In the discussion, we address what we learned about teachers’ pedagogical content knowledge to make computational thinking tangible to students.",Teaching Computational Thinking with Electronic Textiles: High School Teachers’ Contextualizing Strategies in Exploring Computer Science
"This study put four steps, problem decomposition, pattern recognition, abstraction, and algorithm, into practice by integrating the blocky programming language, Scratch, into a mathematics course. The teacher guided the sixthgraders to apply the four steps of computational thinking to writing a blocky program to solve daily-life equality axiom mathematics problems. The results showed that the method was beneficial for promoting the learning effectiveness of mathematics, and also found that there was a significantly positive correlation between the performance of blocky programming and the mathematics post-test. There was no significant correlation between creative tendency and self-efficacy after the experiment. Self-efficacy had a positive correlation to learning motivation both before and after the experiment.",Application of the Four Phases of Computational Thinking and Integration of Blocky Programming in a Sixth-Grade Mathematics Course
"This article documents the design and evaluation of a teacher development programme in computational thinking (CT) education. The results suggested that after taking a teacher development course (TDC), teachers enhanced their CT content knowledge; however, some teachers still did not have sufficient confidence in teaching CT in their classrooms. Subsequent modification to address this lack of confidence among this cohort of teachers is discussed.",The Design and Evaluation of a Teacher Development Programme in Computational Thinking Education
"This current study as part of multi-year design-based research reports our attempt to design and implement a course in teacher education in Korea. We have incorporated design thinking (DT) into the course design and investigated how primary teachers appreciate the role of DT and recognize the connection between teaching computational thinking and DT. This paper reports the course design, its progression, reflections, and learning outcomes.",Connecting Design Thinking and Computational Thinking in the Context of Korean Primary School Teacher Education
"As digital technology is increasingly a part of all sectors of society, educational approaches must be developed in order to nurture students’ ability to see the world through a computational lens. One way to achieve this goal is to promote Computational Thinking (CT) for young learners. The CoolThink@JC project is a four-year curriculum pilot designed to integrate CT into Hong Kong upper-primary level schools. The CoolThink framework for curriculum development is structured around computational concepts, practices and perspectives adapted from the framework of Brennan and Resnick (2012). This adapted framework motivated the choice of learning activities for CoolThink. This paper focuses on one aspect of that framework, namely computational practices. Here, we describe how activities in the CoolThink curriculum can promote the computational practices highlighted by the framework.",Curriculum Activities to Foster Primary School Students’ Computational Practices in Block-Based Programming Environments
"This study presents a sequential analysis of the relationship of emergent roles to student collaboration and computational thinking in the multi-dimensional problem space of educational robotics. The interactions of six groups (n=17) of middle-school aged girls participating in a one-day introduction to robotics workshop were video and audio recorded. Here we analyze one group of three girls’ interactions and the emergence of distinct roles that correlate with periods of collaboration and periods of parallel solo work, which, in turn, impact student’s engagement in computational thinking including solution planning, algorithmic operations, and design of the robotic device. Suggestions for future research are provided.","Emergent Roles, Collaboration and Computational Thinking in the MultiDimensional Problem Space of Robotics"
"Computational Thinking (CT) has become popular in recent years and has been recognized as an essential skill for the digital generation. Students are exposed to computational thinking when they do programming, and MIT App Inventor is currently one of the most popular block based programming environments. Meanwhile, Design thinking is considered as a creative, humancentred, participative, exploratory and problem-solving process that values different perspectives of a problem. In this study, we aim to bring the design thinking in a curriculum framework of K-12 to promote computational thinking by App Inventor. The future work is to implement and evaluate CT curriculum.",A Framework of Computational Thinking Curriculum for K-12 with Design Thinking by App Inverntor
"Programming is one of the important literacies in the digital age. The acquisition of such knowledge and skills is of vital importance to the next generation. This study aimed to develop and validate an instrument to measure programming self-efficacy of senior primary school learners (Grade 4 - Grade 6) in a block-based environment. The proposed scale consisted of two subcomponents related to learners’ perceptions of their own competence in (1) programming knowledge and (2) programming skills. In order to assess the validity of the scale, online questionnaires were distributed to 106 primary school students who joined a course of a new programming curriculum. The objective of the curriculum is to nurture young learners to solve daily life problems. The reliability of the scale was good. The confirmatory factor analysis (CFA) supported the validity of the instrument. More specifically, results indicated that the hypothesized measurement model of the scale fit the data collected. It confirmed that the scale was valid and adequate for measuring programming self-efficacy of senior primary school learners. Theoretical and practical implications of this study were discussed at the end of the paper.",Development and Validation of a Programming Self-Efficacy Scale for Senior Primary School Learners
"Computational thinking is well-known in computer science and is currently entering the field of education. Due to changes in the private and professional life by modern technologies all students are with increasing relevance expected to possess sufficient knowledge in computer-related problem-solving (e.g. Fraillon et al., 2014). The acquisition of key competences related to this assumes an enhancement of knowledge in learning as well as computational thinking processes. Although many concepts for computational thinking education have been created (e.g. Barr & Stephenson, 2011; Krauss & Prottsman, 2017), in fact, an evidence-based competence model is not yet available, thereby it represents a significant desideratum. Considering these aspects, the contribution at hand aims to contribute to this and focuses on the construction and investigation of a model, taken theoretical aspects and the current state of research into account. The principle of this procedure is to break down the term and construct of ‘computational thinking’ to core elements by working with a literacy approach and presuppose that computational thinking can only be implemented in lessons in a competence-oriented way referring to an evidence-based approach to computational thinking as a key competence of the 21st century. Starting from this, the research presented in this paper describes and explains preliminary work in the context of the preparation of IEAICILS 2018 (International Computer and Information Literacy Study). In this context, the authors of this paper are involved as members of the national study center in Germany, which is among other countries taking part in this international study.",Computational Thinking as a Key Competence – a Research Concept
"This study aimed to uncover the underlying mental processes that might facilitate creative thinking after listening to a 10-min music excerpt. A qualitative component was incorporated in a quantitative study regarding the effect of music listening on creative thinking. Among 192 participants, a total of 24 college students were interviewed immediately after they listened to the 10-min music excerpts and completed some creativity tasks. The results suggested a possible facilitative role of music listening on creative thinking through optimizing individuals’ arousal level, and strengthening individuals’ associative abilities, holistic perception and abstraction. As these mental processes are also important attributes of computational thinking (CT), the findings may shed light on the favourable impact of music listening on CT.",Can Music Exposure Enhance Computational Thinking? Insights from the Findings on the Music-Creativity Relations
"The KIBO robotics kit offers a playful and tangible way for young children to learn computational thinking skills by building and programming a robot. KIBO is specifically designed for children ages 4-7 years old and was developed by the DevTech research group at Tufts University through nearly a decade of research funded by the National Science Foundation. KIBO allows young children to become engineers by constructing robots using motors, sensors, and craft materials. Children also become programmers by exploring sequences, loops, and variables. Through programming KIBO, children engage with computational thinking skills and ideas including algorithms, modularity, and control structures. Unlike other programming interfaces for children, the KIBO robot is programmed to move or to respond to sensor input by using tangible programming blocks—no computer, tablet, or screen-time required. This paper provides an overview of the design features of KIBO and a synthesis of the research that has been done throughout the development of this kit. It provides examples of curriculum for playfully engaging young children with computational thinking using KIBO.","Imagining, Playing, and Coding with KIBO: Using Robotics to Foster Computational Thinking in Young Children"
"ScratchJr is a free programming application for young children ages 5-7, available for most tablet devices. This programming environment, developed by the DevTech Research Group at Tufts University, the Lifelong Kindergarten Group at MIT, and the Playful Invention Company, was launched in July, 2014. During the first year after the app’s launch, no information was collected regarding usage other than informal communication with local educators and parents. Starting in January 2016, the ScratchJr team began to use the tool Google Analytics to gain a deeper insight into user behavior, and began to investigate the learning analytics data that could shed light on computational thinking in early childhood. This paper presents the first year of user data collection of ScratchJr",Programming with ScratchJr: a review of the first year of user analytics
"This paper shares the background, vision, the eclectic approach and sharing of the technology-supported initiatives introduced for young children in the network of 129 My First Skool (MFS) childcare centres in Singapore. Focus is given to the communication of a widespread initiative to bring multiple technologies to young children in a large network of schools and systemic provisions for teacher equipping to use these technologies.","Technology Strategy Mapping in My First Skool Childcare Centres, Singapore"
"This paper argues that the various problems caused by the traditional mathematical approach to teaching discrete mathematics to computing students can be alleviated by way of integrating computational thinking into discrete mathematics. The paper proposes a combination of three ideas to facilitate such integration: (a) aiming at understanding the notion of computation, (b) emphasizing both abstraction and automation, and (c) incorporating a functional programming language. The paper exemplifies a plausible approach to developing computational thinking in higher education, namely, through integrating it with an existing subject.",Integrating Computational Thinking into Discrete Mathematics
"The task of learning programming is a complex process that requires students to simultaneously master the syntax and programming tool while applying problem-solving skills to the given situation. Failure to do so have led to students dropping out of computer science programs and students being disenchanted with programming to the point that these graduates are reluctant to practice in the field. To counter this issue, problem-solving training is sometimes introduced before programming to introduce them to primary concepts of program design and programming without the complexity of syntax and tools as a hindrance. However, problem-solving skills is not something that can be developed over a short period of time. For some, it takes time, practice and effort in which semester long courses do not permit. Previous studies have shown that games can be used as educational tools in the classroom. However, video games are frequently overlooked as an educational tool in favor of serious games. In this paper, we analyze the game play of selected game titles to determine if existing video games contain activities that can be associated to each of the five core skills that characterize computational thinking within the Computer Science discipline.",Computational Thinking Affordances in Video Games
"Textile and apparel industry has long been stereotyped as “traditional” and “old-fashioned”. As a non-traditional company in a traditional industry, Esquel encourages employees to innovate and to challenge the status quo. “You can code” campaign was initiated in 2015 to engage and propel staff at all levels towards its vision of “Making a Difference”. The campaign aims to drive a sustained cultural transformation to turn the less technically minded employees into confident users of technology with computational thinking (CT) ability, through developing a mobile apps. Many useful mobile apps have been developed and some have been commercially adopted. The campaign helps the Company to nurture a culture of innovation, problem-solving and collaboration.",You Can Code – An innovative approach to transform the workforce in the textile and apparel industry
"This paper discusses the opportunities presented by the growth of the Internet of Things (IoT) to provide youth opportunities to develop their computational thinking and digital empowerment. This paper argues that to support youth in developing these literacies, we need to develop platforms that reduce the barriers of entry while still allowing them to explore and develop their computational identities. To this end, this paper introduces an extension to App Inventor by MIT that enables students to quickly design, develop, and implement IoT applications. We outline one IoT activity for youth and future directions for both curricular and technical development.","Off the Screen, and Into the World of Everyday Objects: Computational Thinking for Youth with the Internet of Things"
"The Malaysian Ministry of Education aims to increase interest in learning Science, Technology, Engineering and Mathematics, through Science2Action. Among these initiatives in Science2Action, is the use of Art(s). By combining the Internet, technology and crafts, e-crafting is formed. This e-crafting project aims to increase awareness about what interests the audience through sharing of and development of craft, hopefully towards possibilities of ideation and mixing crafts, extending from the original craft such as origami. Designed based on the Technology Acceptance Model, findings are positive.",Developing interest to share and craft based on the Technology Acceptance Model
"Computational thinking (CT) is emerging as a key set of problem-solving skills that must be developed by the new generations of digital learners. However, there is still a lack of consensus on a formal CT definition, on how CT should be integrated in educational settings, and specially on how CT can be properly assessed. The latter is an extremely relevant and urgent topic because without reliable and valid assessment tools, CT might lose its potential of making its way into educational curricula. In response, this paper is aimed at presenting the convergent validity of one of the major recent attempts to assess CT from a summative-aptitudinal perspective: the Computational Thinking Test (CTt). The convergent validity of the CTt is studied in middle school Spanish samples with respect to other two CT assessment tools, which are coming from different perspectives: the Bebras Tasks, built from a skill-transfer approach; and Dr. Scratch, an automated tool designed from a formativeiterative approach. Our results show statistically significant, positive and moderately intense, correlations between the CTt and a selected set of Bebras Tasks (r=0.52); and between the CTt and Dr. Scratch (predictive value r=0.44; concurrent value r=0.53). These results support the statement that CTt is partially convergent with Bebras Tasks and with Dr. Scratch. Finally, we discuss if these three tools are complementary and may be combined in middle school.",Complementary Tools for Computational Thinking Assessment
"This paper introduces the concept of a virtual reality (VR) programming environment that allows youth to both develop immersive VR experiences while enhancing computational thinking (CT). Specifically, we extended a blocks-based programming platform, MIT App Inventor, to allow youth to make VR Android apps (AI/VR). We compare AI/VR's support for CT to other existing VR editors using the CT concepts established by Brennan and Resnick (2012). Comparisons showed that AI/VR’s support for all CT concepts and its ease of use for kids, makes it more preferable for teaching CT compared to other editors.",App Inventor VR Editor for Computational Thinking
"Many countries that recognize the importance of Computational Thinking skills are implementing curriculum changes to integrate the development of these skills and to introduce coding into formal school education. Singapore has introduced new programmes from Pre-school to Secondary children to develop Computational Thinking skills and introduce programming. A major change in the secondary school syllabus is the introduction of a new Computing subject taken at “O” levels. The new subject emphasizes on the development of Computational Thinking skills and coding in Python. Students are expected to apply technology for creating solutions to solve problems. In this paper, we describe the various initiatives in Singapore for Preschool, Primary and Secondary schools. From initiatives in these three school going groups, we review Singapore’s approach to implementation of learning Computational Thinking. Unlike several countries that has decided to implement computing as compulsory education, Singapore has taken a route of creating interest amongst children in Computing in age-appropriate ways. Singapore’s pragmatic approach is characterized by opt-in by schools, nurturing students’ interest in computing, upskilling teachers in computing, and a multi-agency approach.",Computational Thinking and Coding Initiatives in Singapore
"Collaboration becomes increasingly important in programming as projects become more complex. With traditional text-based programming languages, programmers typically use a source code management system to manage the code, merge code from multiple editors, and optionally lock files for conflict-free editing. There is a limited corpus of work around collaborative editing of code in visual programming languages such as block-based programming. We propose an extension to MIT App Inventor, a web-based visual platform for building Android applications with blocks, which will enable many programmers to collaborate in real-time on MIT App Inventor projects. We take the position that real-time collaboration within MIT App Inventor will encourage students in a group environment to interact with one another in ways that help them improve each other’s understanding and practice of computational thinking practices that may not be achieved in the traditional one user-one project paradigm that is currently provided.",Enabling Multi-User Computational Thinking with Collaborative Blocks Programming in MIT App Inventor
"In the context of integrating Computational Thinking (CT) in Primary School Education, we examine the selfdevelopment of undergraduate students during their engagement as Teaching Assistants (TAs) in CT Education. More specifically, we propose to adopt the stress-adaptation-growth process of the Intercultural Transformation Theory (ITT) as a framework to provide evidences of the self-development of TAs in the CoolThink@JC project of Hong Kong. The collected data confirms the evidences of the stress-adaptationgrowth process of TAs engagement, which helps transforming undergraduate students into co-teachers with commitment to future civic involvement.",Evidences of Self-Development of TAs in CT Education
"The evolution of Cloud computing makes the major changes in computing world as with the assistance of basic cloud computing service models like SaaS, PaaS, and IaaS an organization achieves their business goal with minimum effort as compared to traditional computing environment. On the other hand security of the data in the cloud database server is the key area of concern in the acceptance of cloud. It requires a very high degree of privacy and authentication. To protect the data in cloud database server cryptography is one of the important methods. Cryptography provides various symmetric and asymmetric algorithms to secure the data. This paper presents the symmetric cryptographic algorithm named as AES (Advanced Encryption Standard). It is based on several substitutions, permutation and transformation.",Enhancement of Cloud Computing Security with Secure Data Storage using AES
"Cloud computing providers offer two different pricing schemes when renting virtual machines: reserved instances and ondemand instances. On-demand instances are paid only when utilized and they are useful to satisfy a fluctuating demand. Conversely, reserved instances are paid for a certain time period and are independent of usage. Since reserved instances require more commitment from users, they are cheaper than on-demand instances. However, in order to be cost-effective compared to on-demand instances, they have to be extensively utilized. This work focuses on finding the optimal combination of on-demand and reserved instances, such that the demand is satisfied and the costs minimized. To achieve this goal, this study introduces a stochastic model of the resources, based on Inventory Theory. The idea is to formulate the optimization problem as an inventory-keeping problem and then derive the optimal strategy. The paper evaluates the proposed model using data from an industry case, comparing the performance with a brute-force approach. The conducted experiments show that the Inventory Theory model provides accurate results and potentially allows prior research on Inventory Theory to be applied to optimal cloud provisioning.",Inventory Theory Applied to Cost Optimization in Cloud Computing
"Cloud computing has been popular as the IT architecture. Cloud service providers offer many services based on cloud computing. Cloud storage service is the cloud services which can provide a huge storage space to solve the bottleneck of the storage space of local end users. However, cloud storage service may have data security because the users’ data is not stored in their own storage. In this paper, we will focus on data integrity in the cloud storage service. Public auditability is a model of outsourcing data integrity verification, which can achieve efficiency and security. Therefore, we survey the previous researches of data integrity based on public auditability which includes collecting the basic requirements and evaluation metrics, providing the representative with approaches to analyze security and efficiency. Finally, we propose some future developments.",A Survey of Public Auditing for Secure Data Storage in Cloud Computing
"Cloud is a Pool of servers, all the servers are interconnected through internet, The main problem in cloud is retrieving of data (knowledge) and process that variety of data and here other problem is security for that data, Generally now a day?s different types of, I mean variety of data (Structured, semi-structured and Unstructured data) is existed in the different social applications (face book).So, and another problem with historical data retrieving. These types of problems are resolved with help of hadoop frame work and Sqoop and flume tools. Sqoop is load the data from database to Hadoop (HDFS), and flume loads the data from server files to hadoop distributed file system. Storage problem is resolving with help of blocks in hadoop distributed file system and processing is resolving with help of map reduce and pig and hive and spark etc. This paper summarizes the storage and processing speed in the enhanced cloud with hadoop framework.",Storage and Processing Speed for Knowledge from Enhanced Cloud Computing With Hadoop Frame Work : A Survey
"With the help of new technology and methodology it has become possible to manage very large amount of sensing data and apply new integrated computing models to acquire information intelligence This paper elaborates how the Internet Consumer industry has innovated at rapid pace, explores how this. This paper offers a user oriented approach to the specification of requirement for the effective management of urban areas. This paper also explores how such an enterprise transformation has fundamentally changed the business scenario in the telecom industry.",Cloud Computing for Agent-Based Urban Transportation System
"To facilitate extensive collaborations, today’s organizations raise increasing needs for information sharing via ondemand information access. Information Brokering System (IBS) a top a peer-to-peer overlay has been proposed to support information sharing among loosely federated data sources. It consists of diverse data servers and brokering components, which help client queries to locate the data servers. However, many existing IBSs adopt server side access control deployment and honest assumptions on brokers, and shed little attention on privacy of data and metadata stored and exchanged within the IBS. In this article, we study the problem of privacy protection in information brokering process. We first give a formal presentation of the threat models with a focus on two attacks: attribute-correlation attack and inference attack. Then, we propose a broker-coordinator overlay, as well as two schemes, automaton segmentation scheme and query segment encryption scheme, to share the secure query routing function among a set of brokering servers. With comprehensive analysis on privacy, end to- end performance, and scalability, we show that the proposed system can integrate security enforcement and query routing while preserving system-wide privacy with reasonable overhead. Finally, T-broker uses a lightweight feedback mechanism, which can effectively reduce networking risk and improve system efficiency. The experimental results show that, compared with the existing approaches, our T-broker yields very good results in many typical cases, and the proposed system is robust to deal with various numbers of dynamic service behavior from multiple cloud sites.",Secure and Trusted Information Brokering In Cloud Computing
"To plan and improve a fuzzy logic and neural network based trust and reputation model for safe resource allocation in cloud computing is the most important motto of this research. Among the IT professionals in current scenario, the cloud computing is one of the main topics conversed. Now, to revise the security, we employ the trust manager and reputation manager in our proposed approach At first, the user access a resource block through the scheduling manager and a structure will send to the user following accessing the resource block to fill the characteristic values of Trust Factor (TF) and Reputation Factor (RF). The TF and reputation value is after that computed for the resource center and it is specified to the fuzzy logic system and neural network to obtain the Security Score (SS) of a resource center. To offer the security controls is the advantage of our suggested method in accessing the cloud resources from cloud computing owing to different security issues occurred in networks, databases, resource scheduling, transaction management and load balancing",Designing a Fuzzy-Logic Based Trust and Reputation Model for Secure Resource Allocation in Cloud Computing
"Host-side flash caching has emerged as a promising solution to the scalability problem of virtual machine (VM) storage in cloud computing systems, but it still faces serious limitations in capacity and endurance. This paper presents CloudCache, an on-demand cache management solution to meet VM cache demands and minimize cache wear-out. First, to support on-demand cache allocation, the paper proposes a new cache demand model, Reuse Working Set (RWS), to capture only the data with good temporal locality, and uses the RWS size (RWSS) to model a workload’s cache demand. By predicting the RWSS online and admitting only RWS into the cache, CloudCache satisfies the workload’s actual cache demand and minimizes the induced wear-out. Second, to handle situations where a cache is insufficient for the VMs’ demands, the paper proposes a dynamic cache migration approach to balance cache load across hosts by live migrating cached data along with the VMs. It includes both on-demand migration of dirty data and background migration of RWS to optimize the performance of the migrating VM. It also supports rate limiting on the cache data transfer to limit the impact to the co-hosted VMs. Finally, the paper presents comprehensive experimental evaluations using real-world traces to demonstrate the effectiveness of CloudCache.",CloudCache: On-demand Flash Cache Management for Cloud Computing
"Cyber-Physical Systems (CPS) are engineered systems combining computation, communications, and physical resources. The purpose of this chapter is to provide an overview of the emerging field of CPS security. In contrast to the other chapters in this book, which can trace the roots of their fields back to several decades, the work on CPS security is relatively new, and our community has not yet developed the same consensus on best practices. Therefore, in this document we focus on providing an overview of the research trends and unique characteristics in this field.",Cyber-Physical Systems Security
Cloud computing is a rising as a new model of large – scale distributed computing. In these system a large amount of data is used that is distributed between many systems. Dividing the data and allocate them to different systems is the main challenge because the performance of the system has been directly propose to the distributed data. Hear the one method is proposed for managing data distribution called Divisible Load Theory (DLT). Since many years divisible load theory has become a popular area of research. According to the divisible load theory the computations and communications can be divided into some arbitrarily independent parts and each part can be processed independently by a processor. In some situation the fraction of load mast be allocated based on some priorities but some existing divisible load scheduling algorithm do not consider any priority for allocating fraction of load so this paper proposes model that consider many criteria with different priorities for allocating fractions of load to processors. Experimental result indicates that the existing algorithm can handle the priority of processors using the Analytical Hierarchy Process.,A Review on Divisible Load Scheduling and Allocation on Cloud Computing
"With the growing reliance on the Internet, and the devices connected to the global network and the increasing likelihood of attacks and cybercrime is cyber security the essential foundation of e-learning safe, so can not imagine the growth of any information activity away from achieving, and e-learning in the modern time faces many challenges, Perhaps the most important explosion information and knowledge , The most world's universities and all colleges currently provide advanced technological technologies for computer, internet and other multimedia for the development of education, and became the system Blackboard from educational systems which take an important part of the educational technological environment. It is a familiar means for most educational purposes, as well as providing and presenting courses and discussions . It also works to break the Inertia between the teacher and the student, thus developing the educational process and making it more comfortable. employment the e-learning using system safe Blackboard become The language of the age and has a significant impact in improving the quality of education, and communication between students electronically and linking it to cyber security has a significant role in maintaining the development and quality of e-learning, hence the importance of research has reached several results and recommendations was one of the most important: The linking of e-learning (Blackboard) to cybersecurity creates a safe education and provides a great opportunity to give the opportunity to education to many groups of society, providing data and information that circulates through networks for e-learning makes it more effective, As a Blackboard Collaborate tool to provide online lessons and lectures as it transcends places and times intervals, it helped distance training and spread education and made learners able to learn very effectively.",E-Learning using the Blackboard system in Light of the Quality of Education and Cyber security
"The electric grid is one of the mostcomplex and important infrastructures ever created, and is vital to modern quality of life and the economy. Generation of electricity is also a significant source of greenhouse gas emissions. Modernization of the grid is central to many nations’ efforts to address climate change and improve energy efficiency and reliability. The smart grid represents the integration of information and communications technologies into the existing power system to provide measurement and control needed for increased use of distributed and renewable generation, enabling dynamic management of demand as well as generation, improving reliability, and support for electric vehicles. Keywords- Interagency Report (IR), Field Message Bus (FMB), Federal Information Processing Standard(FIPS)",CYBER SECURITY IN SMART GRID
"Nowadays cyber security is becoming an ever more stringent requirement and warned by organizations and companies all over the world. Furthermore, the educational offer on the topic is still modest and universities are struggling to design training courses capable of producing professionals directly employable. In this work, this need is addressed with the proposal of an integrated model ""The Hack Space"", developed within the Master of Science in Computer Security of the University of Bari, composed of four main elements: Organization, Knowledge, Skills/Tools and Collaboration. The Hack Space aims to create professionals capable of dealing with security at various levels, with clear ideas on what are the processes, functions and controls useful for security, using an in-depth knowledge structure of the company.",Teaching Cyber Security: The Hack-Space Integrated Model
"Mobile cloud computing in light of the increasing popularity among users of mobile smart technology which is the next indispensable that enables users to take advantage of the storage cloud computing services. However, mobile cloud computing, the migration of information on the cloud is reliable their privacy and security issues. Moreover, mobile cloud computing has limitations in resources such as power energy, processor, Memory and storage. In this paper, we propose a solution to the problem of privacy with saving and reducing resources power energy, processor and Memory. This is done through data encryption in the mobile cloud computing by symmetric algorithm and sent to the private cloud and then the data is encrypted again and sent to the public cloud through Asymmetric algorithm. The experimental results showed after a comparison between encryption algorithms less time and less time to decryption are as follows: Blowfish algorithm for symmetric and the DSA algorithm for Asymmetric. The analysis results showed a significant improvement in reducing the resources in the period of time and power energy consumption and processor.",Reduce Resources for Privacy in Mobile Cloud Computing Using Blowfish and DSA Algorithms
"Education is highly important in today's society. It helps to motivate the minds and shape it into intellectuals. Many Academic Institutes are exploring new technologies for effective teaching and learning Methodology. One of the emerging technologies Cloud Computing can be very useful in teaching learning process. As Cloud provides a variety of services, an institute can offer quality education by providing latest infrastructure in terms of hardware and software. This paper focuses on basic introduction of cloud Computing and how cloud computing can be introduced in the education to improve teaching and learning methodology which can bring a revolution in the field of education",Cloud Computing in Education Sector
"As the world context changed dramatically after cold war, global phenomena are being appeared continually. The cyber space, since it arose, together with land space, sea space, air space and outer space, constituted five power spaces, and naturally become a game space for nations. However, features of virtual cyber space are not same as other entities’ features. Hence, this study starts with elaboration current cyber security situation faced by China and US, then, ensures what kind cyber security dilemma China and US in and reveals driving forces behind China and US cyber security dilemma. Further, the study analyses the nature of cyber space which cau",The Origin of Security Dilemma between China and US in Cyber Space
"Global Data Fusion is one of the main technologies used in complex systems. While the application of data fusion has already been proposed for the implementation of specific tools, its extension to the overall design process of a complex system is far from a desired target. The development of advanced architectures based on an interdisciplinary design approach makes this extension possible, especially at the higher levels of the architecture, involving situation assessment, impact assessment and process refinement. This paper analyses one of the advanced cyber security architectures and explores the capability of this architecture to include data fusion tools at the top level of the architecture. The effects of the generalisation of data fusion techniques are then analysed and the consequent improvements in the network security of critical infrastructures are described and quantified.",The Notion of Global Data Fusion and its Application to Cyber Security
"In past decade machine learning (ML) and deep learning (DL), has generated irresistible research interest and attracted unprecedented public attention. With the increasing integration of the Internet and social life, there is change in how people learn and work, but it also exposes them to serious security threats. It is a challenging task to protect sensitive information, data, network and computers connected systems from the unauthorized cyberattacks. For this purpose, effective cyber security is required. Recent technologies such as machine learning and deep learning are integrated with cyberattacks to provide solution to this problem. The paper surveys machine learning and deep learning in cyber security also it discusses the challenges and opportunities of using ML / DL and provides suggestions for research directions.",A survey of cyber security operations based on Machine learning & Deep learning
"Data deduplication is the technique which compresses the data by removing the duplicate copies of identical data and it is extensively used in cloud storage to save bandwidth and minimize the storage space. To secure the confidentiality of sensitive data during deduplication, the convergent encryption technique is used to encrypt the data before outsourcing. For better data protection, this paper talks about the issue of data deduplication authorization. There are several new deduplication implementations providing authorized deduplication verification in a hybrid cloud approach.",A Survey on Authorized Deduplication Techniques in Cloud Computing
"The evolution to network and computational technologies has gone through a remarkable phase of growth and development. The growth curve was indeed very steep in major domain of application of these technologies. The advent of Cloud computing, Big Data analytics, Evolutionary computing, Internet of Things (IoT) etc. has enhanced the implementation avenues of these technologies in various application areas. Cloud computing has emerged as a special area of interest for many researchers keeping in view its huge application-domain scope. Research is being done on different aspects of CC for identifying areas of improvement and their respective remedies. One important issue in CC is that of Security, because of the various threats of working on network architecture. This paper scribbles through various review papers and research papers to identify the threats and security requirements for different levels of use and the corresponding users. It reviews the perspective of the users at various level already described in good quality research papers which highlight security requirements and tries to emphasize on threats faced by users for those security requirements as mix of required security consideration can be better defined if the threats involved are taken into account and their respective remedies can be designed keeping user perspective in mind and their cloud computing usage experiences can be enhanced by following appropriate security measures.",User centric security requirements and threat analysis in Cloud Computing
"Natural hazards are severe events that pose a threat to the sustainment and survival of our society. Every year extreme weather and climate events, such as typhoons, floods, tornadoes, hurricanes, volcanic eruptions, earthquakes, heat waves, droughts, or landslides, claim thousands of lives, cause billions of dollars of damage to property (Smith and Matthews, 2015) and severely impact the environment worldwide (Velev and Zlateva, 2012). Natural hazards become disasters when they cause extensive damage, casualties, and disruption (Vasilescu et al., 2008). Disasters have been increasing in both frequency and severity in the 21st century because of climate change, increasing population, and reliance on aging infrastructure. Recently, major events have caused havoc around the world, such as the 2015 earthquakes in Nepal, the 2015 heat wave in India, the 2011 tsunami in Japan, the 2010 earthquake in Haiti, and the extremely cold winter of 2014/2015 in the United States and in Europe",Usage of Social Media and Cloud Computing During Natural Hazards
"Computing resources in cloud computing with heterogeneous, dynamic, non-balancing and other features, how to allocate resources to fully improve resource utilization and reduce task execution time and energy consumption optimization is the key problem to be faced in cloud computing. According to the basic characteristics of task scheduling in cloud computing environment, this paper proposes an energy consumption optimization model for task scheduling, and proposes a green clonal scheduling optimization algorithm by taking advantage of the clonal operator of immune algorithm. Experimental results show that the proposed algorithm can not only effectively reduce the execution time and energy consumption, and can achieve resource load balancing, thus effectively improve the resource utilization and scheduling efficiency.",A Parallel Task Scheduling Optimization Algorithm Based on Clonal Operator in Green Cloud Computing
"Cloud computing research area has just come up as a latest prototype for delivering, hosting services, in which common utilities (CPU, Storage) are provided to users which can be rented and freed in an on-demand manner over the internet. Even though, it provides various characteristics like on-demand supply of utilities or resources, multi-tenancy, decreased cost, agility etc., and also associated flaws and risks with it. A variety of research issues areas are associated with it and fault tolerance (FT) is one of them. It is the procedure of detecting failures and faults and if a fault takes place due to the hardware/software failure afterward the cloud computing system must also perform correctly. In real time applications, delay in processing due to fault is not accepted in cloud. By using virtualization technique, high availability of resources with minimum down time is achieved. To facilitate this feature a method is used to discover failover in physical servers which further detects failure in the host. FT is essential for the system to assurance both guaranteed availability & continuous reliability of critical application and services execution. So in this, robust Fault Tolerant (RoFT) system is required. To understand fault tolerance in it, it is necessary to know more about various types of failure/faults. Our emphasize in this paper is on essential FT concepts by knowing its policies namely; RFTP (Reactive), PFTP (Proactive) and some related procedures or methods apply on different failures or faults. A lot of research on different FT frameworks, algorithms, methods that are implemented, designed by professional has been accomplishing.",A Comparative Review on Fault Tolerance methods and models in Cloud Computing
"Recent years all are using the cloud server. The reason of using cloud server is easy to use and having the security. With the rapid development of cloud services, lots of new challenges have emerged. One of the most important problems is how to delete the outsourced and shared data which is permanently stored in the cloud storage with secure. In our system, We propose a novel KP-TSABE scheme which is able to achieve the time specified cipher text in order to solve these problems by implementing flexible fine-grained access control during the authorization period and time-controllable self-destruction after expiration to the shared and outsourced data in cloud computing. We also specified a system model and security model for the KP-TSABE scheme. Furthermore, we proved that KP-TSABE is secure under the standard model with decision 1-Expanded BDHI assumption. The comprehensive analysis indicates that the proposed KP-TSABE is superior to other existing schemes.",A Protected Data Self-Slaughter in Cloud Computing
"Agile global software development (AGSD) is an increasingly prevalent software development strategy, as organizations hope to realize the benefits of accessing a larger resource pool of skilled labor, at a potentially reduced cost, while at the same time delivering value incrementally and iteratively. However, the distributed nature of AGSD creates geographic, temporal, socio-cultural distances that challenge collaboration between project stakeholders. The Cloud Computing (CC) service models of Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) are similar to the aspirant qualities of AGSD as they provide services that are globally accessible, efficient, and stable, with lower predictable operating costs that scale to meet the computational demand. This study focused on the 12 agile principles upon which all agile methodologies are based, therein potentially increasing the potential for the findings to be generalized. Domestication Theory was used to assist in understanding how cloud technologies were appropriated in support of AGSD. The research strategy took the form of case study research. The findings suggest that some of the challenges in applying the agile principles in AGSD may be overcome by using CC.",Cloud Computing as an Enabler of Agile Global Software Development
"Rapid growth of the demand for computational power has led to the creation of large-scale cloud computing data centers.The development of computing systems has always been focused on performance improvements driven by the demand of applications from consumer, scientific and business domains, but the ever increasing energy consumption of computing systems has started to limit further performance growth due to overwhelming energy consumption and carbon dioxide footprints. Hence, the goal of the computer system design has been shifted from performance improvements to power and energy efficiency. Data centers consume enormous amounts of electrical power resulting in high operational costs and carbon dioxide emissions. Moreover, modern Cloud computing environments have to provide high Quality of Service (QoS) for their customers resulting in the necessity to deal with power-performance trade-off. Reducing carbon emission by cloud computing data centers has emerged as one the dominant research topics both in industry and academia. The foremost objective of cloud service providers is to have a cost effective and energy efficient solution for allocating virtualized ICT resources to end-users‘ application while meeting the QoS (Quality of Service) level as per SLA (Service Level Agreement).This paper presents a hybrid energy efficient resource allocation technique which combines predictive with reactive allocation techniques and accomplishes substantial improvements in: (a) meeting SLAs, (b) conserving energy, and (c)meeting static and dynamic resource allocation. In this paper we propose energy-aware allocation heuristics provision data center resources to client applications in a way that utilises the capability of VMs live migration to reallocate resources dynamically and improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). The basic idea is to use a heuristic that is consolidating and rearranging the allocation of resources in an energy efficient manner.",A Pragmatic Approach to Optimize Energy Efficient Resource Allocation Technique in Cloud Computing Data Center
"Pocket electronic dictionaries are a relatively new type of dictionary, having been in existence for just a few years, and their range and quality are increasing. This is a study of their use by students in a tertiary education institution in Hong Kong. It outlines the features of the dictionaries, describes their use, and gives students' views. While students are aware of the weaknesses of these dictionaries, they are attracted by their portability, speed and relative ease of use, and also the availability ofsound. Interviews with teachers indicate that they are more concerned about the weaknesses, although the potential for the use of these dictionaries in language learning is recognized.",Pocket Electronic Dictionaries and their Use
"Background: Pacemaker pocket early post-implantation inflammation (EPII) is defined as clinical signs of local erythema without any systemic signs of infection occurring early after implantation. Data on the best treatment regimen for apparent superficial EPII is scarce. Objectives: To investigate the prognostic value of medical treatment, rather than extraction surgery, in patients with EPII. Methods: Data from 6013 consecutive patients who underwent cardiac implantable electronic device (CIED) implantation or replacement from 2007–2015 were retrospectively analyzed; 40 (0.7%) presented with EPII. Our goal was the absence of major complications and to avoidance of extraction. Results: Patients with EPII were initially treated medically. Nineteen (47%) (group A) recovered with antibiotic treatment only. In the other 21 patients (53%) (group B), CIED extraction was performed. Group B had more major complications compared to group A (15 [71%] vs. 0 [0%], P < 0.001). The only significant difference in baseline characteristics was history of non-initial procedure. While 86% of group B patients had a previous non-initial procedure, only 53% of group A patients underwent previous replacement (P < 0.05). In multivariate analysis, previous non-initial procedure was the only predictor for need of extraction at 1 year, hazard ratio 3.5, 95% confidence interval 1.001–11.73, P < 0.05. Conclusions: Conservative treatment in patients with EPII after non-initial procedure can lead to serious adverse events resulting in the need for extraction. Close follow-up and aggressive treatment should be considered early in the therapeutic course.",Post-cardiac Implantable Electronic Devices: Inflammation of the Pocket. Should We Be More Aggressive?
"MARLIN/Electronic Logsheet terminal designed to record and transmit daily logsheet data electronically was installed to all Philippine Flagged Fishing Vessels Licensed to Operate in HSP1. This paper presents the current status of reporting in terms of the data gathered, frequency of reporting and its reporting processes. Eighteen (18) out of twenty-two (22) vessels operating in HSP1 was able to transmit data for 2013 and twenty nine (30) out of thirty-five (35) fishing vessels for 2014. Data reported is similar with the normal logsheet which includes, vessels departure date and time, position, daily activity, catch species and amount, carrier name and fish hold number used. E-reporting approach can timely provide information that can be advantageous in generating data for immediate for evaluation. However, there is still a need to capacitate the Boat Captains/Officers in performing electronic catch reporting to sustain effective electronic data recording and its reliability",Pilot Test of MARLIN(Electronic Logsheet) Operation in High Seas Pocket 1
"The MSIL to JavaScript compiler is a program that translates MSIL programs into equivalent JavaScript programs can be executed within a browser. This is important because of the new found interesting in using JavaScript to develop AJAX applications that target the browser. The problem is that JavaScript development tools are not suited for building large-scale AJAX applications. The MSIL to JavaScript compiler is an effort to bolster JavaScript development efficiency and ease by allowing developers to use the rich set of development tools available for the .NET platform, and having the resulting program run as a JavaScript/AJAX application. MSIL is the intermediary “byte-code” language used by the .NET platform.",MSIL to JavaScript Compiler
" In  this  paper,  we  present  a  prototype tool  that  automates  the  process  of  detecting, gathering  and  visualizing  multi-language  software metrics  at  an  intermediate-language  level.  More specifically, the current version of our tool focuses on code written using the Microsoft Visual Studio .NET software  development  environment.  It  facilitates  the process  of  locating  and  extracting  software  metrics found  at  the  MSIL  (Microsoft  Intermediate Language) level. We illustrate the basic functionality of  our  tool  and  we  discuss  a  preliminary  case  study performed  in  order  to  verify  its  functionality  and validate  its  usefulness.  Based  on  the  results  of  this study, we continue improving the tool.   Our  broader  research  goal  is  to  show  that complexity  analysis  of  multi-language  software, when  it  is  done  at  an intermediate  language  level, it can be as effective as when conducted at the level of each  individual  language.  This  will  eventually eliminate  the  need  for  developing  different  syntax parsers  for  each  programming  language  used  to develop  multi-language software. The prototype tool described  in  this  paper  is  the  first  step  towards accomplishing such an objective.",A Metrics Tool for Multi-language Software
"The AOSD tools and methodologies have developed in a rapid speed in Java area. However, no matter how homologous .Net and Java are, AOP tools on .Net plat- form are still in experimental stage. The complex infrastructure and multi-language support make it hard to establish aspect-oriented programming on .Net. Microsoft provides Microsoft Intermediate Language(MSIL) to facilitate language implemen- tation on .Net platform. But MSIL doesn't support aspect-oriented languages. To facilitate aspect-oriented language implementation on .Net platform, it is best to provide an aspect-oriented intermediate language. This paper presents CCC, an aspect-oriented intermediate language on .Net plat- form. CCC stands for Common language Cross Cutter. Any aspect-oriented .Net language could first be transformed to CCC and then woven by CCC compiler. Since CCC supports high-level aspect-oriented language constructs directly, such as aspect, there is a little work to do for language implementation and the developers could concentrate on language design.",An Aspect-Oriented Intermediate Language on .Net Platform
"In this paper, we define a hardware Evaluation Stac k for MSIL and we propose a conservative implementation o ver the EPIC architecture. This new hardware evaluation stack, named virtual stack, is based upon the EPIC archite cture's register files. An additional register renaming log ic, to offload the run-time type checking performed by the Common Language Runtime is also proposed. Finally, we introduce a mechanism to overcome the sequential na ture of the evaluation stack allowing the code generator to better use the parallel instructions execution of instruct ion bundles. The virtual stack's final purpose is to si mplify the implementation of fast one-pass JIT compiler.",An MSIL Hardware Evaluation Stack for EPIC
": The purpose of the research is the comparison between the extracting time of the Database Management System (DMS) and XML files for two different platforms - .NET Framework 1.1 and .NET Framework 2.0. The two latest versions of the Microsoft database server are used as Database Management System – SQL Server 2000 and SQL Server 2005. The extraction from the XML file is performed in two different ways – by SAX and DOM parser. The fact, that the typical for Java SAX parser is not realized in .NET, is taken into account, but with some approximation the consecutive reading of elements of the XML file can be used. A special application is developed for testing. The business layout of the application is presented by two web services, respectively working on ASP .NET 2.0 and on ASP .NET 1.1.",Analysis of the Benchmark while Extracting Data from Database or XML File for Different Platforms
"Part of the new .NET platform from Microsoft is the new programming language C# which was first presented in 1999. In many articles, C# was compared with Java and C++, and its new and novel features were presented and discussed. In this article we do not want to elaborate these language extensions (as e.g. properties, events, attributes, operator overloading, etc.) but rather concentrate on some subtle and almost imperceptible language changes in which C# differs from Java. These language refinements enable the compiler to mark potential problems which are otherwise only found by static analyzer tools (as e.g. lint). Programmers can no longer fall into prominent language trap doors and thus save development and debugging time. We hope, that some of these language enhancements find their way back to the Java language.",C # and Java: The Smart Distinctions
"In this approach the .NET framework is used to implement functionality that is rapidly becoming commonplace in the GRID computing and agent communities, capabilities like web service description and remote method invocation. The .NET framework builds upon established standards such as XML (Extended Markup Language) and WSDL (Web Service Description Language) which can be used to describe and publicize robot and agent capabilities. It also supports SOAP (Simple Object Access Protocol) and has built in capabilities for routing and handling service requests over a network using HTTP. Also important is the fact that the .NET framework defines a device independent intermediate language, MSIL, analogous to Java bytecodes which gives us a convenient mechanism for shipping executable content around a network.",A Programming Platform for Distributed Robots based on Microsoft’s .NET Framework
"The use of Aspect Orientation has the potential to increase the separation of concerns and to identify crosscutting aspects. The Composition filters approach to AOP is based on filtering of messages between objects. This allows for more flexible and reusable aspects, in comparison to other approaches like AspectJ. The Microsoft .NET framework is centered around the Common Language Infrastructure. All .NET supported languages are mapped to one common intermediate language. The result is a language independent programming environment where one language can call functions and methods of other languages. The main goal of the Compose* project is to incorporate the composition filters on the .NET intermediate language. With one effort all .NET supported languages are thus made aware of aspects. The combined result is a language independent aspect oriented approach on a language independent programming environment. For instance an aspect written in C# and being imposed on a FORTRAN base system.",Detecting semantic conflicts between aspects
"Modern computer systems depend on many pieces of software gathered together to perform certain activities. That is why various forms of distributed systems are being developed where individual software components come from different developers. Such distributed systems bring a lot of freedom and convenience but when misused they can do a lot of damage. Thus, there is a deep need for various kinds of safety and security at different levels of a software life-cycle. In this work we investigate some notions of safety taking the safety based on contracts into special consideration. We build eXtensible Multi Security - a framework based on the notion of Proof Carrying Code which is a powerful and coherent platform able to unify various notions of safety. We also show how XMS forms a certification framework for Microsoft Intermediate Language and other programming languages of the .NET Platform.",eXtensible Multi Security: Infrastruktura bezpieczenstwa dla platformy .NET
"Fuzzy modelling of power systems can take in account the qualitative aspects and vagueness or uncertainty that have not a random nature and therefore cannot be modelld by a probabilistic approach. This paper presents new fuzzy load flow analysis tools which enable one to incorporate in power system modelling information which is not deterministic nor stochastic about loads and generated powers, often expressed in a qualitative or linguistic way, and obtain results under the form of fuzzy information about voltages, active and reactive power flows and losses. To achieve this, a conceptual framework appropriate to assess the possibility of events is adopted. New fuzzy DC and AC formulations are presented, and the results under the form of possibility distributions obtained for an application study are examined. Adequate linearizing techniques have been adopted to overcome the difficulties related to some special cases where the resulting possibility distributions for currents and losses are described by membership functions that greatly differ from the ones assumed as data.",FUZZY LOAD FLOW - NEW ALGORITHMS INCORPORATING UNCERTAIN GENERATION AND LOAD REPRESENTATION
"Nonlinear devices are being increasingly used in power systems and, as a consequence, harmonic distortion is a continuously growing phenomenon. Since linear and nonlinear loads are usually uncertain variables that, in addition, vary constantly, probabilistic methods for harmonic loadflow calculation have been developed to quantify the resulting random harmonic voltages. More recently, a possibilistic harmonic load-flow based on fuzzy sets theory has been proposed with the same objective. Although the possibilistic approach seems better suited to deal with the kind of uncertainty usually found in practice, the Classic Fuzzy Solution on which this methodology relies exhibits some drawbacks. This paper proposes a possibilistic harmonic load-flow based on the Marginal Joint Solution, which overcomes two major limitations of the Classic Fuzzy Solution approach: the lack of fuzzy linear loads models, and the over-and underestimation of the harmonic voltages.","A FUZZY NUMBER BASED METHODOLOGY FOR HARMONIC LOAD-FLOW CALCULATION, CONSIDERING UNCERTAINTIES"
Load flow calculation is one of the most basic problems in power engineering. The repetitive solution of a large set of linear equations in load flow problem is one of the most time consuming parts of power systems simulations. Load flows are calculated using the traditional method such as Gauss Seidel or Newton Raphson methods. Gauss Seidel algorithm is an iterative numerical procedure and in this method the number of iteration depends on the acceleration factor (?). Here we attempt to choose the acceleration factor (?) using fuzzy logic technique so as to minimize the number of iteration and get the result in minimum required time. Comparison of the method with traditional method has been shown in the paper which proves the validity of the result. The paper shows as to how the application of Fuzzy technique in choosing an appropriate acceleration factor reduces the number of iteration and helps in obtaing the solution at a faster rate with optimum number of iterationThe simulation in carried out in MATLAB environment.,A Fuzzy Based Efficient Load Flow Analysis
"Instability and voltage collapse problem are two common problems in modern distribution systems. The most important task for distribution system is to efficiently simulate the system, identify the sensitive nodes and address the uncertainty using a suitable mathematical model. The paper presents a Fuzzy modeled solution for three phase Radial Power Distribution System (RDS) and simulates Voltage Stability Index (SI) for all the nodes so that the conditions of voltage collapse can be known real time. Uncertainties in Load & Line parameters are modeled as Fuzzy functions, hence results obtained incorporates the uncertainty, can be used as an early warning and is a useful input for planning purposes",Fuzzy Modeled Load Flow Solution for Unbalanced Radial Power Distribution System
"`Abstract: - This paper presents a new approach to solve the load-flow problem using Tanaka's Fuzzy Linear Regression formulation (FLR). The load-flow model is formulated as a fuzzy linear optimization problem, where the objective is to minimize the sum of the spread of the states, subject to double inequality constraints on each pre-specified active and reactive power to guarantee that the original membership is included in the state membership. Linear programming is employed to obtain the middle and the symmetric spread for every state variable. The estimated middle corresponds to the value of the state. While the symmetric spreads in the membership functions of the state variables represents the uncertainty (vagueness) around the state. The proposed formulation has been applied to various test systems. The outcome is very encouraging and proves that proposed (FLR) is very applicable and shows reliability, accuracy in solving the power-flow problem.",Solution of Load-Flow Problem using Fuzzy Linear Regression Approach
"This paper proposes the application of the fuzzy arithmetic algorithm (FAA) in the analysis of radial distribution system power flow in the presence of uncertainties. FAA takes care of the uncertainty in the input parameters and provides strict bounds for the solution of the problem. The sources of uncertainties of the electric quantities of the distribution network are the variation in the temperature which modifies the parameters of the lines, the variation of the tap change of the transformers which modifies the output voltage, the variation in the temperature during the seasons which varies the values of the loads and the input parameters wind and photovoltaic farms such as the speed of the wind and the solar radiations. In this paper all these uncertainties are incorporated using a fuzzy distribution-based fuzzy arithmetic algorithm. IEEE 33-bus test system is used to demonstrate the effectiveness of the proposed algorithm. The results are compared with those obtained from load flow simulations found in a recent publication.",Analysis of Radial Distribution System Load Flow under Uncertainties with Fuzzy Arithmetic Algorithm
"Load flow analysis uncertainty treatment via fuzzy arithmetic is a method which applying fuzzy arithmetic to model vagueness, ambiguity and uncertainties in power system analysis. In this study, trapezoidal method and transformation method have been employed to solve the uncertainties in load flow analysis where LR (left-right) fuzzy arithmetic have been applied to model the uncertainties and another method is by composing fuzzy numbers into intervals by using transformation method. The fuzzy load flow has been performed to get the ouput results. The simulation study is conducted for IEEE 5-bus test system and IEEE 9-bus test system using both methods. The output voltage of the fuzzy load flow has been plotted against their membership function to validate the result in terms of fuzzy distribution and to compare which method is more efficient as solution for load flow uncertainties treatment.",LOAD FLOW ANALYSIS UNCERTAINTY TREATMENT VIA FUZZY ARITHMETIC
"The researchers in electrical energy systems are always developing new models for the electrical networks, trying to incorporate the technicians’ and engineers’ knowledge in computational algorithms to get results as near as of the reality. We applied fuzzy numbers (bell shape) to represent imprecise variables and we present a new technique to electrical distribution system load flow based on fuzzy sets. With this new computational algorithm is possible to deal with information like the voltage in the node k is high and the results can also be interpreted through linguistics terms.",A New Technique to Electrical Distribution System Load Flow Based on Fuzzy Sets
"In 2004, Ken Jennings set a record for the largesttotal winnings and longest undefeated streak onthe television game show Jeopardy ($2.5 millionand 74 days, respectively). In 2011, IBM invitedJeopardy to present the show from its arti?cialintelligence (AI) base in Westchester, NY, andpitted Jennings and another big winner, MattRutter, against Watson, the company’s AI system.Watson utilizes natural language processing (NLP)capabilitiesdwhich enabled it to analyze thequestions posed by Alex Trebekdand has thememory, processing, and machine learning (ML)powers required to compete against humans withtremendous access to general knowledge. Watsonemerged the overwhelming winner of the 2-daytournament. As part of his Final Jeopardyresponse, Jenningsdin a humorous nod to Wat-son’s dominancedadded: “I, for one, welcome ournew computer overlords.",Artificial intelligence and machine learning: What managers need to know
"Depression is a leading source of medical disability and is experienced by over 322 million people worldwide. Despite its increasingly significant burden and a pressing need for effective treatment, depression has been persistently difficult to treat. Current common practice for treatment selection is an educated guess-and-check approach, in which clinicians prescribe one of the numerous approved therapies for depression in a stepwise manner. Though evidence-based clinical guidelines for managing depression exist, there is a paucity of evidence to support specific treatment recommendations. A significant barrier to developing such recommendations is the symptom heterogeneity present in the diagnosis of major depressive disorder. Machine learning offers the ability to recognize this heterogeneity and model that information in psychiatric disorders. Specifically, machine learning allows processing of high-dimensional data, the management of missing data, creating highlevel abstractions, and the freedom of not requiring a priori patient stratification. While ethical concerns arise in employing these methods, the benefits are wide-reaching, from personalizing treatment for depression, to the development of artificial intelligence chats that employ psychotherapy, to predicting social outcomes for patients with mental illness. The implications extend far beyond depression treatment, as the epidemiology and service demand for mental healthcare systems continue to grow. Indeed, psychiatry is primed for innovation in artificial intelligence and machine learning",Primed for Psychiatry: The role of artificial intelligence and machine learning in the optimization of depression treatment
"This report provides an overview of some promising applications of artificial intelligence (AI) in German universities. Although the AI sector is booming, the higher education sector seems to have benefited little from this boom thus far. In any case, schools and universities are relatively low-priority targets in the development of new AI-based systems than, for example, medical diagnostics or individual transport. This report aims to provide initial insights into the relevant applications that are currently being developed at and for universities in Germany in this opaque scenario. To this end, the report was prepared based on a methodological triangulation. In the first step, relevant literature on AI in the university sector and existing state-of-the-art reports of other countries were evaluated. In the second step, an analysis of the official documents of German universities pertaining to AI and digitization strategies was carried out, as far as such papers were available. In the third step, 13 guideline-based expert interviews were conducted to confirm and extend the impressions gained from the relevant literature and from the document analysis. On this empirical basis, a few of the AI systems currently in use at tertiary education institutions are presented, the opportunities and risks associated with their use are discussed, and the future of such systems is discussed. Even if we cannot claim that this report provides a complete picture of the domain, it does highlight important fields of application and lines of development related to AI.",Machine Learning and Artificial Intelligence in Higher Education: A State-of-the-Art Report on the German University Landscape
"This paper discusses selected publications that relate cyber-security to artificial intelligence (AI) in general and to machine learning (ML) specifically. The focus is cyber-security in the context of software development and life-cycle environments (SDLE) and their products. Because of the large volume of publications in this area, the survey includes publications that are themselves surveys of specialized subjects. A few papers are cautionary, pointing out that systems trained using AI/ML can be fooled; one of these papers investigates methods for the design of classifiers that exhibit resilience to adversarial actions and points to other literature in this emerging field. Several references are books, and the discussions of these provide some guidance for those who might be interested in further reading across the breadth of the field. A few relevant publications from the NIST 800 series are included as background to provide an appropriate setting for the discussion of AI/ML as applied to cyber-security.",Utility of Artificial Intelligence and Machine Learning in Cybersecurity
"The future of any business from banking, e-commerce, real estate, homeland security, healthcare, marketing, the stock market, manufacturing, education, retail to government organizations depends on the data and analytics capabilities that are built and scaled. The speed of change in technology in recent years has been a real challenge for all businesses. To manage that, a significant number of organizations are exploring the Big Data (BD) infrastructure that helps them to take advantage of new opportunities while saving costs. Timely transformation of information is also critical for the survivability of an organization. Having the right information at the right time will enhance not only the knowledge of stakeholders within an organization but also providing them with a tool to make the right decision at the right moment. It is no longer enough to rely on a sampling of information about the organizations' customers. The decision-makers need to get vital insights into the customers' actual behavior, which requires enormous volumes of data to be processed. We believe that Big Data infrastructure is the key to successful Artificial Intelligence (AI) deployments and accurate, unbiased real-time insights. Big data solutions have a direct impact and changing the way the organization needs to work with help from AI and its components ML and DL. In this article, we discuss these topics.",Artificial Intelligence Driven Resiliency with Machine Learning and Deep Learning Components
"There have been theory-based endeavours that directly engage with AI and ML in the landscape discipline. By presenting a case that uses machine learning techniques to predict variables in a coastal environment, this paper provides empirical evidence of the forthcoming cybernetic environment, in which designers are conceptualized not as authors but as choreographers, catalyst agents, and conductors among many other intelligent agents. Drawing ideas from posthumanism, this paper argues that, to truly understand the cybernetic environment, we have to take on posthumanist ethics and overcome human exceptionalism.","The Future of Artificial Intelligence (AI) and Machine Learning (ML) in Landscape Design: A Case Study in Coastal Virginia, USA"
"This paper is aimed to explore the application of Artificial Intelligence/Machine Learning (AI/ML) to software engineering research problems. Which activities of software engineering use AI/ML the most for solving research problems? The scope of the paper is to preliminary review research papers published in Asia-Pacific Software Engineering Conference 2018 (APSEC 2018) proceedings and researches conducted at the Department of Computer Engineering, Chulalongkorn University (CPCU). The author manually reviews papers with some keywords such as machine learning, neural network, and natural language processing. The result shows that machine learning is used in coding and software quality improvement activities more than other activities.",Literature Reviews on Applying Artificial Intelligence/Machine Learning to Software Engineering Research Problems: Preliminary
"The thesis is one of the requirements to complete the lecture for the S1 level. One of the universities that hold an undergraduate academic level is STMIK Sumedang, but if we look at STMIK Sumedang which is the campus of Informatics Engineering and Information Technology in terms of its technology utilization is not optimal, one that has not used technology is the Thesis process carried out in STMIK Sumedang. Starting from filing the title until the Thesis process is complete all the processes are still conventional which requires a lot of time, paper material and no real-time guidance can be made. Through this research, the author performs Analysis and Design to make all the Thesis processes automated. This Thesis automation system uses several techniques and methods needed in software design. For the system data flow method using an object-oriented method that is using UML in describing use case diagrams, Activity diagrams, Sequence diagrams, from the results of research that has been conducted on the problems found, the system can be developed using CodeIgniter (PHP framework). The use of this framework was chosen because the media can be accessed through all devices so that it is easier to deliver information more effectively. From the results of this study produce automation systems that can save time and money, all related elements will feel benefited, because this automation system covers the entire Thesis process from the submission of the title until the trial process is completed until the judicial process.",Thesis Process Automation System
"—Business process automation is an important task in an enterprise business environment software development. The requirements of processing acceleration and automation level of enterprises are inherently different from one organization to another. We present a methodology and system for automation of business process management system architecture by multi-agent collaboration based on SOA. Design layer processes are modeled in semantic markup language for web services application. At the core of our system is considering certain types of human tasks to their further automation across over multiple platform environments. An improved abnormality processing with model for automation of BPMS architecture by multi-agent collaboration based on SOA is introduced. Validating system for efficiency of process automation, an application for educational knowledge base instance would also be described.",Multi-Agent Model for Automation of Business Process Management System Based on Service Oriented Architecture
"Nippon Steel Corporation led the steel industry in adopting general-purpose personal computers and operating systems as process computer systems for automatic optimal control and developing the middleware NS SEMI SYSTEM in the activity of open system solution. The enhancement of middleware function has made the extension of adopting process field. Nowadays, the renewals of process computers projects mostly have adopted the middleware and great cost reduction has been achieved. The open system application technology including PLC and DCS fields is accumulating and the middleware and support tools originally developed as well are shared with all works. The middleware has been adopted not only in the steel industry including overseas but also in other industries. To keep up with various needs quickly, Nippon Steel Corporation is promoting open system solution activities and development on the latest open system technology.",Application of Open System to Process Automation in Ironand-steel making
"Collectively, process simulation is an intricate element within an intelligent transport system (ITS); arrange of modeling techniques and subsequent transition operations also form components of such a system. Although the ITS incorporates asset of process automations such as systematic prototypes and simulation, it also integrates computational modeling, and therefore, efficient operational transitions. Processes such as automation, modeling, and operational transitions are at the core of a fastidious ITS. Accordingly, these system elements need to be holistically integrated and amalgamated through effective computationbased methods. Fittingly, the main objective of this study is to examine the utilization of process automation- and computation-based methods as the basis for ITS integration perspectives. In doing so, Sydney Metro will be included as a case study to further elucidate such integration processes.",Process Automation in Intelligent Transportation System (ITS)
"Building production process has a complex nature and fragmented structure due to the characteristics of project type productions, i.e., number of participants, various organizational patterns, multi-phased production etc. Some of the currently available information system solutions and models in both conceptual and practical dimensions try to provide an integration and continuity through the phases of building production process in terms of various management-related functions by integrating the components that correspond to these functions. This paper presents an integrated design of a unified computer-based office automation system, MITOS (Multi-phase Integrated Automation System for Construction Industry) that basically comprises three relational database models; ASAP (Automation System for Architectural Practices); ASCC (Automation System for Construction Companies) and ASCE (Automation System for Cost Estimation).",MITOS: Multi-phase Integrated Automation System for Building Production Process
"This project reviewed technologies that could lead to greater construction efficiency by integrating life-cycle stages of construction including conception, design, construction, and operations and maintenance, and by improving the flow of resources. The research consulted stakeholders to determine current practices and to gauge the need for process automation and integration. The report assesses changes that might include tracking the location and status of materials, equipment, personnel, and other resources that would facilitate delivery on demand. Dans le cadre de ce projet, on a examiné certaines technologies qui pourraient accroître l'efficacité de la construction en intégrant tous les stades du cycle de vie de la construction, dont la conception, l'avant-projet, la construction ainsi que l'exploitation et l'entretien, et en améliorant le flux des ressources. Aux fins de la recherche, on a consulté les intéressés pour déterminer les pratiques courantes et évaluer les besoins en matière d'automatisation des procédés et d'intégration. Dans le rapport, on évalue différentes modifications potentielles, notamment le suivi de l'emplacement et de l'état des matériaux, de l'équipement et du personnel ainsi que des autres ressources qui faciliteraient la livraison sur demande. RES",Need for Intelligent System for Construction Process Automation
"The complexity and variability of the Electric Arc Furnace process call for advanced monitoring and control solutions, in order to improve process performances, environmental compatibility and operator safety. The innovative Danieli Q-MELT Automatic EAF system addresses these aspects implementing a centralized control system which interacts with multiple technological packages. Each of these cutting-edge technologies focuses on one aspect of the EAF cycle, increasing machine availability and resource efficiency, by means of electrode regulation and foamy slag control, charging optimization, off-gas analysis and closed loop injectors control, on-line temperature measurement, automatic tapping and robotics. Moreover, these modules provide the process supervisor application (Melt Model) with important information regarding the process status, thus allowing the adoption of a unified control strategy. The supervisor implements a robust statistical approach to identify process deviations in real time. By means of advanced data collecting and data mining techniques, the available process data are clustered and filtered, and the expected trends of the key process variables are thus extracted. Melt Model applies this data-driven adaptive strategy to the oxygen injection management during the refining phase, optimizing the decarburization process and increasing the furnace performances. With such a modular and adaptive approach, Q-MELT aims to the “Zero Operator on Melting Floor” practice and allows multiple levels of process optimization, from a single process aspect to the entire EAF cycle.",EAF process optimization through a modular automation system and an adaptive control strategy
"Today’s power plants are highly automated. All subsystems of large thermal power plants can be controlled from a central control room. One subsystems area is the electrical auxiliaries for the unit transformer, the grid connection, excitation, synchronization, generator/unit protection, auxiliary transformers, HV-, MV- and LV-switchgear. In the past, these electrical devices were all hardwired to the DCS and I/Os. To this day, horizontal communication between electrical devices is still hardwired. In the last decade, serial communication protocols were introduced. Unfortunately, standardization of these protocols went in different directions. Today there are several standards on the market. Most of them are incompatible with others. In substation automation, the newly developed IEC 61850 standard was released in 2004. IEC 61850 defines interoperable function blocks, called logical nodes, that communicate over a network with other functions, regardless on which suppliers' device they are implemented. IEC 61850 separates application and communication layers, which enables one part to be changed without impacting the other part. New future communication technologies can be easily adapted to IEC 61850 which will make applications future proof over their service life. In future, it will be possible to modify or extend electrical without replacing the entire substation automation system. IEC 61850 is expected to be a future standard in power plant electrical systems, at least for HV and MV. It will have applicationspecific extensions; e.g., IEC 61850-7-420 (formerly IEC 62344) for hydro power applications. This results in new requirements for state-of-the art power plant control systems. Users can seamlessly monitor and operate electrical devices the same way they do the process control. This paper discusses various aspects of the implementation of the standard in power plants and shows how devices integrate into ABB's control system environment.",Future power plant control - Integrating process & substation automation into one system
"Process automation has been one of the fastest developing fields of process industry in recent years. This makes for quite high demands on automationpersonnel. They should master the process they are supervising. The high level of automation can, however, reduce the skills of the personnel to copewith tasks the automation system cannot do. The recent development in automation systems and networks enables supply  of information through netsto support learning during the work. This paper presents a learning environment to be developed in the Control Engineering Laboratory at theUniversity of Oulu. The environment consists of a pilot scale rotary dryer, relating process automation system, and the network connections to localnetwork and Internet. The paper will also describe how earlier, in a COMETT II project, developed hypermedia material is going to be utilized in thisenvironment. It will also outline problems that may be confronted while integrating different systems together and building this kind of  learningenvironment",A Learning Environment For a Process Automation System
